<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>神经网络初识 | 全之の博客</title><meta name="author" content="ZU Quanzhi"><meta name="copyright" content="ZU Quanzhi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="参考资料：新手入门python实现神经网络，超级简单!_py 神经网络-CSDN博客 📎Python神经网络编程.pdf Python学习篇30-神经网络_python神经网络-CSDN博客 “反向传播算法”过程及公式推导（超直观好懂的Backpropagation）-CSDN博客 反向传播算法推导过程（看一篇就够了）_神经网络反向传播算法推导-CSDN博客"><meta property="og:type" content="article"><meta property="og:title" content="神经网络初识"><meta property="og:url" content="http://blog.zuquanzhi.top/2024/01/12/65251/index.html"><meta property="og:site_name" content="全之の博客"><meta property="og:description" content="参考资料：新手入门python实现神经网络，超级简单!_py 神经网络-CSDN博客 📎Python神经网络编程.pdf Python学习篇30-神经网络_python神经网络-CSDN博客 “反向传播算法”过程及公式推导（超直观好懂的Backpropagation）-CSDN博客 反向传播算法推导过程（看一篇就够了）_神经网络反向传播算法推导-CSDN博客"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.loliapi.com/acg?10"><meta property="article:published_time" content="2024-01-12T15:31:21.000Z"><meta property="article:modified_time" content="2025-09-17T13:30:58.693Z"><meta property="article:author" content="ZU Quanzhi"><meta property="article:tag" content="神经网络"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://www.loliapi.com/acg?10"><link rel="shortcut icon" href="/img/logo.png"><link rel="canonical" href="http://blog.zuquanzhi.top/2024/01/12/65251/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: ZU Quanzhi","link":"链接: ","source":"来源: 全之の博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"神经网络初识",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2025-09-17 21:30:58"}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/transpancy.css"><link rel="stylesheet" href="/css/mouse.css"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="全之の博客" type="application/atom+xml"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/head.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">52</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">41</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/cloud/"><i class="fa-fw fas fa-cloud"></i><span> Cloud</span></a></div><div class="menus_item"><a class="site-page" href="/Pictures/"><i class="fa-fw fas fa-images"></i><span> Pictures</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://www.loliapi.com/acg?10)"><nav id="nav"><span id="blog-info"><a href="/" title="全之の博客"><span class="site-name">全之の博客</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/cloud/"><i class="fa-fw fas fa-cloud"></i><span> Cloud</span></a></div><div class="menus_item"><a class="site-page" href="/Pictures/"><i class="fa-fw fas fa-images"></i><span> Pictures</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">神经网络初识</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-12T15:31:21.000Z" title="发表于 2024-01-12 23:31:21">2024-01-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-17T13:30:58.693Z" title="更新于 2025-09-17 21:30:58">2025-09-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">机器人/计算机视觉</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="神经网络初识"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h4 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_22583741/article/details/129444508?ops_request_misc=%7B%22request%5Fid%22%3A%22170486960516800197010429%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=170486960516800197010429&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-129444508-null-null.142^v99^pc_search_result_base6&amp;utm_term=python神经网络&amp;spm=1018.2226.3001.4187">新手入门python实现神经网络，超级简单!_py 神经网络-CSDN博客</a></p><p><a target="_blank" rel="noopener" href="https://hitwhlc.yuque.com/attachments/yuque/0/2024/pdf/39221021/1704869715414-7f00ccda-2452-42d9-a816-4a09a794e0ad.pdf">📎Python神经网络编程.pdf</a></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40221426/article/details/132255702?ops_request_misc=%7B%22request%5Fid%22%3A%22170486960516800197010429%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=170486960516800197010429&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-5-132255702-null-null.142^v99^pc_search_result_base6&amp;utm_term=python神经网络&amp;spm=1018.2226.3001.4187">Python学习篇30-神经网络_python神经网络-CSDN博客</a></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/ft_sunshine/article/details/90221691?ops_request_misc=%7B%22request%5Fid%22%3A%22170487093716800192217002%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=170487093716800192217002&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-90221691-null-null.142^v99^pc_search_result_base6&amp;utm_term=反向传播算法&amp;spm=1018.2226.3001.4187">“反向传播算法”过程及公式推导（超直观好懂的Backpropagation）-CSDN博客</a></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/baidu_41774120/article/details/125764136?ops_request_misc=%7B%22request%5Fid%22%3A%22170487093716800192217002%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=170487093716800192217002&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-125764136-null-null.142^v99^pc_search_result_base6&amp;utm_term=反向传播算法&amp;spm=1018.2226.3001.4187">反向传播算法推导过程（看一篇就够了）_神经网络反向传播算法推导-CSDN博客</a></p><span id="more"></span><h2 id="1-发展历史"><a href="#1-发展历史" class="headerlink" title="1.发展历史"></a>1.发展历史</h2><p>神经网络的发展历史可以追溯到20世纪中叶。</p><ol><li><strong>McCulloch和Pitts的神经元模型（1943）：</strong></li></ol><p>​ Warren McCulloch和Walter Pitts提出了神经元模型，将神经元抽象为二进制开关，形成了后来神经网络的基础。</p><ol><li><strong>感知器的提出（1957）：</strong></li></ol><p>​ Frank Rosenblatt提出了感知器，这是一种基于神经元模型的学习算法。感知器可以实现简单的二分类任务。</p><ol><li><strong>早期神经网络的研究（1960s-1970s）：</strong></li></ol><p>在这一时期，神经网络受到了关注，但受到了硬件和理论上的限制。神经网络的训练和应用遇到了困难。</p><ol><li><strong>反向传播算法的提出（1986）：</strong></li></ol><ul><li>David Rumelhart、Geoffrey Hinton和Ronald Williams提出了反向传播算法，为多层神经网络的训练提供了有效的方法。这一突破重新激发了对神经网络的研究兴趣。</li></ul><ol><li><strong>计算能力的提升（1990s）：</strong></li></ol><p>随着计算能力的提高，研究者们开始更深入地研究神经网络的理论和应用。但由于数据集和计算资源的限制，发展相对较慢。</p><ol><li><strong>深度学习的崛起（2000年后）：</strong></li></ol><ul><li>随着大规模数据集和强大的计算能力的可用性，深度学习（深度神经网络）再次引起了广泛关注。</li></ul><ul><li>图像分类、语音识别、自然语言处理等领域的成功应用推动了神经网络的发展。</li></ul><ol><li><strong>卷积神经网络（CNN）和循环神经网络（RNN）的出现：</strong><ul><li>2012年，AlexNet的成功标志着卷积神经网络（CNN）的兴起，对图像处理任务取得了巨大成功。</li><li>循环神经网络（RNN）在处理序列数据（如自然语言）方面表现出色，为更多领域的应用提供了解决方案。</li></ul></li></ol><ol><li><strong>深度学习在各领域的广泛应用：</strong></li></ol><ul><li>深度学习技术在计算机视觉、自然语言处理、语音识别、医学影像等领域取得了显著的进展，推动了人工智能的发展。</li></ul><h2 id="2-主体流程"><a href="#2-主体流程" class="headerlink" title="2.主体流程"></a>2.主体流程</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="1.png" alt="1"></p><p><strong>输入层（Input Layer）：</strong></p><ul><li>神经网络的第一层，负责接收输入数据。每个输入节点对应输入数据的一个特征。</li></ul><p><strong>权重和偏差（Weights and Biases）：</strong></p><ul><li>每个连接（神经元之间的连接）都有一个权重，用于调整输入的影响力。每个神经元还有一个偏差，用于调整整体激活的阈值。</li></ul><p><strong>线性组合（Linear Combination）：</strong></p><ul><li>输入层的每个神经元将其输入与相应的权重相乘，然后将所有乘积相加，再加上偏差。这形成了线性组合。</li></ul><p><strong>激活函数（Activation Function）：</strong></p><ul><li>线性组合的结果通常通过激活函数，如Sigmoid、ReLU等，以引入非线性特性。这使得神经网络能够学习非线性关系。这个函数有点像高中学过的神经元阈值（神经网络似乎就是模拟人脑结构），当输入信号或信号组合以某种方式达到一定阈值，才会激活这个节点。</li></ul><p><strong>隐藏层（Hidden Layers）：</strong></p><ul><li>在输入层和输出层之间的层称为隐藏层。神经网络的深度取决于隐藏层的数量。每个隐藏层的神经元接收前一层的输出，并重复之前的步骤。</li></ul><p><strong>输出层（Output Layer）：</strong></p><ul><li>最后一个隐藏层的输出作为神经网络的最终输出。输出的数量通常取决于任务类型，如二分类问题有一个输出节点，多分类问题有多个输出节点。</li></ul><p><strong>损失函数（Loss Function）：</strong></p><ul><li>损失函数度量神经网络输出与真实标签之间的差异。训练过程的目标是最小化损失函数。</li></ul><p><strong>优化算法（Optimization Algorithm）：</strong></p><ul><li>优化算法，如梯度下降，用于调整权重和偏差，以降低损失函数。通过计算损失函数关于权重和偏差的梯度，优化算法更新参数。</li></ul><p><strong>反向传播（Backpropagation）：</strong></p><ul><li>反向传播算法是训练神经网络的关键步骤。通过计算梯度，反向传播从输出层到输入层反向传播误差，并更新权重和偏差。</li></ul><p><strong>训练和预测（Training and Prediction）：</strong></p><ul><li>神经网络通过多次迭代训练数据来学习权重和偏差。在训练后，神经网络可以用于对新数据的预测。</li></ul><p><strong>正则化和调参（Regularization and Hyperparameter Tuning）：</strong></p><ul><li>为了提高泛化能力，可以使用正则化技术，并调整超参数（如学习率、隐藏层节点数）。</li></ul><h2 id="3-矩阵的应用"><a href="#3-矩阵的应用" class="headerlink" title="3.矩阵的应用"></a>3.矩阵的应用</h2><h5 id="学习资料与平台"><a href="#学习资料与平台" class="headerlink" title="学习资料与平台"></a>学习资料与平台</h5><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1bx411M7Zx/?t=6&amp;spm_id_from=333.1350.jump_directly&amp;vd_source=a29f1a7926ed643fe5076ad7bf93a8f2">【官方双语】深度学习之神经网络的结构 Part 1 ver 2.0_哔哩哔哩_bilibili</a></p><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Ux411j7ri/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=a29f1a7926ed643fe5076ad7bf93a8f2">【官方双语】深度学习之梯度下降法 Part 2 ver 0.9 beta_哔哩哔哩_bilibili</a></p><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV16x411V7Qg/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=a29f1a7926ed643fe5076ad7bf93a8f2">【官方双语】深度学习之反向传播算法 上/下 Part 3 ver 0.9 beta_哔哩哔哩_bilibili</a></p><p>（作为计算机专业小白常常听说线性代数在计算机领域尤其重要，今天才真正理解）。</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="2.png" alt="2"></p><p>​ 在神经网络的构建中，存在着巨量的参数权重，每一层的数据都代表着运算量进一步飙升。矩阵作为一种数据间关系表达的优良方式（其实就是数表），可以极大程度的简化运算，并能非常简易地表示出结果。下例：</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="3.png" alt="3"></p><h2 id="4-梯度下降"><a href="#4-梯度下降" class="headerlink" title="4.梯度下降"></a>4.梯度下降</h2><h4 id="梯度："><a href="#梯度：" class="headerlink" title="梯度："></a>梯度：</h4><ul><li><strong>梯度</strong>表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（梯度的方向）变化最快，变化率（梯度的模）最大，<strong>可理解为导数</strong>。</li><li>梯度上升和梯度下降是优化算法中常用的两种方法，主要目的是通过迭代找到目标函数的最大值和最小值。</li><li><p>例如：</p></li><li><ul><li>想象我们在一座很高的山上，怎么才能以最快的速度下山？我们可以先选择坡度最倾斜的方向走一段距离，然后再重新选择坡度最倾斜的方向，再走一段距离。以此类推，我们就可以以最快的速度到达山底。<strong>（梯度的方向，就是我们要选择的方向）</strong></li></ul></li></ul><h4 id="梯度下降法："><a href="#梯度下降法：" class="headerlink" title="梯度下降法："></a>梯度下降法：</h4><p><strong>梯度下降算法</strong>针对的是最小优化问题(即求最小值问题)，目的是使目标函数沿最快路径下降到最小值。</p><p>通俗的解释，是模拟下山，每次沿着当前位置最陡峭最易下山的方向前进一小步，然后继续沿下一个位置最陡方向前进一小步。这样一步一步走下去，一直走到觉得我们已经到了山脚。</p><p>算法作用于损失函数(也称目标函数、代价函数、误差函数)，是为了找到使损失函数取最小值的权重(w)和偏置(b)。</p><p>梯度下降运行步骤：</p><ol><li>用随机值初始化权重和偏差</li><li>把输入传入网络，得到输出值(预测值)</li><li>计算预测值和真实值(标签值)之间的误差</li><li><strong>对每一个产生误差的神经元，调整相应的（权重和偏差）值以减小误差</strong></li><li>重复迭代，直至得到网络权重和偏差的最佳值</li></ol><p><strong>批量梯度下降法(BGD)</strong>：每次迭代计算梯度，使用整个数据集。每次更新都会朝着正确的方向进行，最后能够保证收敛于极值点，凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点，缺陷就是学习时间太长，消耗大量内存。</p><p><strong>随机梯度下降法(SGD)</strong>：每次迭代计算梯度，从整个数据集中随机选取一个数据，所以每次迭代的时间非常快。但收敛时震荡，不稳定，在最优解附近波动，难以判断是否已经收敛。</p><p><strong>小批量梯度下降法(MBGD)</strong>：这个是 <strong>BGD</strong> 和 <strong>SGD</strong> 的折中方法， <strong>BGD</strong> 每次使用整体数据，收敛太慢， <strong>SGD</strong> 每次只使用一条数据，虽然收敛快但震荡厉害，所以出现了折中的 <strong>MBGD</strong>，每次使用 <strong>n</strong> 条数据，如果 <strong>n(batch size)</strong> 选择的合适，不仅收敛速度比SGD更快、更稳定，而且在最优解附近的震荡也不会很大，甚至得到比 <strong>BGD</strong> 更好的解。</p><p><strong>batch size</strong> 的选择，一般取2的幂次时能充分利用矩阵运算操作，因此可以在2的幂次中挑选最优取值。例如16、32、64、128、256等等。</p><h2 id="5-反向传播"><a href="#5-反向传播" class="headerlink" title="5.反向传播"></a>5.反向传播</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>首先来一个<strong>反向传播算法</strong>的定义（转自维基百科）：<strong>反向传播</strong>（英语：<strong>Backpropagation</strong>，缩写为<strong>BP</strong>）是“<strong>误差反向传播</strong>”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。 该方法对网络中<strong>所有权重</strong>计算损失函数的梯度。 这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。（<strong>误差</strong>的反向传播）</p><p>首先拿一个简单的三层神经网络来举例，如下：<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="20190515095532783.png" alt="20190515095532783"></p><h5 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h5><p>每个神经元由两部分组成，第一部分（e）是<strong>输入值</strong>和<strong>权重系数</strong>乘积的<strong>和</strong>，第二部分（f(e)）是一个<strong>激活函数</strong>（非线性函数）的输出， y=f(e)即为某个神经元的输出，如下：<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="20190515100159284.png" alt="20190515100159284"></p><p>下面是<strong>前向传播</strong>过程：<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="20190515100805671.png" alt="20190515100805671"><br>—————-手动分割—————-<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="20190515100845442.png" alt="20190515100845442"><br>—————-手动分割—————-<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="20190515101005589.png" alt="20190515101005589"></p><p>到这里为止，神经网络的前向传播已经完成，最后输出的y就是本次前向传播神经网络计算出来的结果（预测结果），但这个预测结果不一定是正确的，要和真实的标签（z）相比较，计算预测结果和真实标签的误差（δ \deltaδ），如下：<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="20190515101232916.png" alt="20190515101232916"></p><p>下面开始计算每个神经元的误差（δ \deltaδ）：<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="20190515101334960.png" alt="20190515101334960"></p><h5 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h5><p>下面开始利用反向传播的误差，计算各个神经元（权重）的导数，开始反向传播修改权重。</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="20190515103014208.png" alt="20190515103014208"></p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="20190515103033715.png" alt="20190515103033715"></p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="20190515103116521.png" alt="20190515103116521"></p><h4 id="“正向传播”求损失，“反向传播”回传误差。"><a href="#“正向传播”求损失，“反向传播”回传误差。" class="headerlink" title="“正向传播”求损失，“反向传播”回传误差。"></a><strong>“正向传播”求损失，“反向传播”回传误差</strong>。</h4><p><strong>BP算法，也叫**</strong>δ \delta<strong><strong>δ</strong></strong>算法**，下面以3层的感知机为例进行举例讲解。<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="20190515104858190.png" alt="20190515104858190"></p><p>上图的前向传播（网络输出计算）过程如下：（此处为网络的整个误差的计算，误差E计算方法为mse）</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="20200331121115812.png" alt="20200331121115812"></p><p>上面的计算过程并不难，只要耐心一步步的拆开式子，逐渐分解即可。现在还有两个问题需要解决：</p><ol><li>误差E有了，怎么调整权重让误差不断减小？</li><li>E是权重w的函数，何如找到使得函数值最小的w。</li></ol><h4 id="（其实这些稀奇古怪的公式已经看不懂了）"><a href="#（其实这些稀奇古怪的公式已经看不懂了）" class="headerlink" title="（其实这些稀奇古怪的公式已经看不懂了）"></a>（其实这些稀奇古怪的公式已经看不懂了）</h4><p>通俗来讲，反向传播就是根据计算结果的误差修改权重信号，俗称打哪指哪，像极了我强行凑答案的样子。通过这种方法可以优化神经网络的整体权重布局，从而使训练结果更上一层楼（也可能中间层的黑箱子照着无法预测的方向走远但是结果准确率超高）</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>综上所述感觉神经网络就是人们用计算机模拟人脑神经元链接构建出来的玩意（果然生物结构才是顶级码农的创造）。对于深度学习，神经网络属于机器学习的一部分，更具体地说是深度学习的一种。机器学习是一种让计算机从数据中学习的方法，而深度学习则是机器学习中的一个分支，强调使用深层次的神经网络结构。</p><p>对于神经网络是否智能这件事，我也说不好，感觉相对人脑来说这种结构实在是太简单了。（比如说计算机永远无法理解我怎么眼睁睁地看着自己把7x8算成45）。目前神经网络似乎缺少了较高程度的自我学习能力，例如发现个什么定律啥的，（照《终结者》一比还是差着层次），但是chatgpt确实很香，看起来AI方面的发展还是很大的，贾维斯指日可待（乐）。</p><h2 id="操作实例"><a href="#操作实例" class="headerlink" title="操作实例"></a>操作实例</h2><p>直接上代码吧，内容都写在注释里了</p><h5 id="神经网络类："><a href="#神经网络类：" class="headerlink" title="神经网络类："></a>神经网络类：</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 神经网络类定义</span></span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> scipy.special</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">neuralNetwork</span>:</span><br><span class="line">    <span class="comment"># 初始化神经网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inputnodes, hiddennodes, outputnodes, learningrate</span>):</span><br><span class="line">        <span class="comment"># 设置每个层中的节点数</span></span><br><span class="line">        self.inodes = inputnodes</span><br><span class="line">        self.hnodes = hiddennodes</span><br><span class="line">        self.onodes = outputnodes</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用随机值初始化权重</span></span><br><span class="line">        self.wih = numpy.random.normal(<span class="number">0.0</span>, <span class="built_in">pow</span>(self.hnodes, -<span class="number">0.5</span>), (self.hnodes, self.inodes))</span><br><span class="line">        self.who = numpy.random.normal(<span class="number">0.0</span>, <span class="built_in">pow</span>(self.onodes, -<span class="number">0.5</span>), (self.onodes, self.hnodes))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设置学习率</span></span><br><span class="line">        self.lr = learningrate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义激活函数为sigmoid函数</span></span><br><span class="line">        self.activation_function = <span class="keyword">lambda</span> x: scipy.special.expit(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练神经网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, inputs_list, targets_list</span>):</span><br><span class="line">        <span class="comment"># 将输入和目标转换为2维数组</span></span><br><span class="line">        inputs = numpy.array(inputs_list, ndmin=<span class="number">2</span>).T</span><br><span class="line">        targets = numpy.array(targets_list, ndmin=<span class="number">2</span>).T</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算隐藏层的输入和输出信号</span></span><br><span class="line">        hidden_inputs = numpy.dot(self.wih, inputs)</span><br><span class="line">        hidden_outputs = self.activation_function(hidden_inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算输出层的输入和输出信号</span></span><br><span class="line">        final_inputs = numpy.dot(self.who, hidden_outputs)</span><br><span class="line">        final_outputs = self.activation_function(final_inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算输出层的误差</span></span><br><span class="line">        output_errors = targets - final_outputs</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算隐藏层的误差</span></span><br><span class="line">        hidden_errors = numpy.dot(self.who.T, output_errors)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用学习率和梯度更新权重</span></span><br><span class="line">        self.who += self.lr * numpy.dot((output_errors * final_outputs * (<span class="number">1.0</span> - final_outputs)), numpy.transpose(hidden_outputs))</span><br><span class="line">        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (<span class="number">1.0</span> - hidden_outputs)), numpy.transpose(inputs))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查询神经网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">query</span>(<span class="params">self, input_list</span>):</span><br><span class="line">        <span class="comment"># 将输入转换为2维数组</span></span><br><span class="line">        inputs = numpy.array(input_list, ndmin=<span class="number">2</span>).T</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算隐藏层的输入和输出信号</span></span><br><span class="line">        hidden_inputs = numpy.dot(self.wih, inputs)</span><br><span class="line">        hidden_outputs = self.activation_function(hidden_inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算输出层的输入和输出信号</span></span><br><span class="line">        final_inputs = numpy.dot(self.who, hidden_outputs)</span><br><span class="line">        final_outputs = self.activation_function(final_inputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回网络的输出</span></span><br><span class="line">        <span class="keyword">return</span> final_outputs</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data_file=<span class="built_in">open</span>(<span class="string">&quot;/home/zuquanzhi/pythonProject/mnist_dataset/mnist_train.csv&quot;</span>,<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">data_list=data_file.readlines()</span><br><span class="line">all_values=data_list[<span class="number">0</span>].split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">image_array=numpy.asfarray(all_values[<span class="number">1</span>:]).reshape((<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">matplotlib.pyplot.imshow(image_array,cmap=<span class="string">&#x27;Greys&#x27;</span>,interpolation=<span class="string">&#x27;None&#x27;</span>)</span><br></pre></td></tr></table></figure><ol><li><strong>data_file=…</strong>：打开 MNIST 训练数据集文件，并将其内容读取到 <strong>data_list</strong> 列表中。</li><li><strong>data_list=data_file.readlines()</strong>：读取文件的所有行，并存储在 <strong>data_list</strong> 列表中。</li><li><strong>all_values=data_list[0].split(‘,’)</strong>：将第一行数据按逗号分隔，存储到 <strong>all_values</strong> 列表中。</li><li><strong>image_array=numpy.asfarray(all_values[1:]).reshape((28,28))</strong>：将 <strong>all_values</strong> 中的像素值转换为浮点数，并重新形状为 28x28 的数组，表示图像的像素矩阵。</li><li><strong>matplotlib.pyplot.imshow(image_array,cmap=’Greys’,interpolation=’None’)</strong>：使用 Matplotlib 的 <strong>imshow</strong> 函数将图像以灰度的形式显示在图像窗口中。</li></ol><p>这段代码读取 MNIST 数据集中的第一张图像数据，并显示在图像窗口中。</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="a350e116ec9ac187600f639bb8574216.png" alt="a350e116ec9ac187600f639bb8574216"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data_file=<span class="built_in">open</span>(<span class="string">&quot;/home/zuquanzhi/pythonProject/mnist_dataset/mnist_train.csv&quot;</span>,<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">data_list=data_file.readlines()</span><br><span class="line">all_values=data_list[<span class="number">5</span>].split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">image_array=numpy.asfarray(all_values[<span class="number">1</span>:]).reshape((<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">matplotlib.pyplot.imshow(image_array,cmap=<span class="string">&#x27;Greys&#x27;</span>,interpolation=<span class="string">&#x27;None&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>data_list[5]</strong> 表示第6行数据，<strong>all_values=data_list[5].split(‘,’)</strong> 将这一行数据根据逗号分割成一个值的列表。然后，将列表中除第一个值外的其余值转换为浮点数数组，并reshape为28x28的图像矩阵，最后使用<strong>matplotlib</strong>库将图像显示在灰度色图上。</p><p>(其实就是根据数据显示原图)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scaled_input=(numpy.asfarray(all_values[<span class="number">1</span>:]) /<span class="number">255.0</span>*<span class="number">0.99</span>)+<span class="number">0.01</span></span><br><span class="line"><span class="built_in">print</span>(scaled_input)</span><br></pre></td></tr></table></figure><p>这段代码对MNIST数据集中第6行的像素值进行了预处理。首先，<strong>numpy.asfarray(all_values[1:])</strong> 将第6行除第一个值外的其余值转换为浮点数数组。然后，通过除以255.0将像素值缩放到0到1之间，接着将数据范围缩放到0.01到1之间，而不是0到1。这是因为在神经网络中，为了避免输入值为0的情况（可能会影响权重的更新），将数据范围设置为稍微偏离0的范围。</p><p>最后，通过<strong>print(scaled_input)</strong> 将缩放后的输入值打印出来，以便查看处理后的数值范围和数据。</p><p>这段代码可以帮助确保在使用神经网络之前对输入数据进行了适当的预处理，以提高神经网络的训练效果。</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="416a55bcc508c87e6a28fb7d848dcec6_720.png" alt="416a55bcc508c87e6a28fb7d848dcec6_720"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">onodes=<span class="number">10</span></span><br><span class="line">targets=numpy.zeros(onodes)+<span class="number">0.01</span></span><br><span class="line">targets[<span class="built_in">int</span>(all_values[<span class="number">0</span>])]=<span class="number">0.99</span></span><br></pre></td></tr></table></figure><p>这段代码是为了准备目标输出。它创建了一个长度为<strong>onodes</strong>的零数组（全零数组），然后将第<strong>int(all_values[0])</strong>个位置设置为0.99。这个位置对应于<strong>all_values[0]</strong>中的值，通常表示图像中显示的数字。<strong>int(all_values[0])</strong>的值被用作索引，用于将目标输出数组中对应的位置值设置为0.99。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data_file=<span class="built_in">open</span>(<span class="string">&quot;/home/zuquanzhi/pythonProject/mnist_dataset/mnist_test.csv&quot;</span>,<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">data_list=data_file.readlines()</span><br><span class="line">all_values=data_list[<span class="number">0</span>].split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">image_array=numpy.asfarray(all_values[<span class="number">1</span>:]).reshape((<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">matplotlib.pyplot.imshow(image_array,cmap=<span class="string">&#x27;Greys&#x27;</span>,interpolation=<span class="string">&#x27;None&#x27;</span>)</span><br><span class="line"></span><br><span class="line">all_values=test_data_list[<span class="number">0</span>].split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(all_values[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="1014148cd6ee8d823f12ab6de2c2ea0b_720.png" alt="1014148cd6ee8d823f12ab6de2c2ea0b_720"></p><ol><li><strong>data_file=open(“/home/zuquanzhi/pythonProject/mnist_dataset/mnist_test.csv”,’r’)</strong>: 打开了一个名为<strong>mnist_test.csv</strong>的CSV文件来读取测试数据。</li><li><strong>data_list=data_file.readlines()</strong>: 读取CSV文件的内容，并将每一行数据存储在<strong>data_list</strong>列表中。</li><li><strong>all_values=data_list[0].split(‘,’)</strong>: 从第一行提取数据，使用逗号作为分隔符将数据拆分为一个值的列表<strong>all_values</strong>。</li><li><strong>image_array=numpy.asfarray(all_values[1:]).reshape((28,28))</strong>: 将<strong>all_values</strong>列表中的字符串转换为浮点数，并根据这些值创建一个28x28的二维数组 <strong>image_array</strong>，用于表示图像的像素值。</li><li><strong>matplotlib.pyplot.imshow(image_array,cmap=’Greys’,interpolation=’None’)</strong>: 使用Matplotlib库中的<strong>imshow</strong>函数将<strong>image_array</strong>作为灰度图像显示出来。</li><li><strong>all_values=test_data_list[0].split(‘,’)</strong>: 这里应该更正为<strong>data_list</strong>而不是<strong>test_data_list</strong>，以便使用刚刚加载的测试集数据。这行代码意图是重新读取<strong>data_list</strong>的第一行数据并将其拆分。</li><li><strong>print(all_values[0])</strong>: 打印 <strong>all_values</strong> 列表的第一个值，这可能是与图像相关联的标签或类别。</li></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://blog.zuquanzhi.top">ZU Quanzhi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://blog.zuquanzhi.top/2024/01/12/65251/">http://blog.zuquanzhi.top/2024/01/12/65251/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://blog.zuquanzhi.top" target="_blank">全之の博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></div><div class="post_share"><div class="social-share" data-image="https://www.loliapi.com/acg?10" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/01/12/63833/" title="CMake入门"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://www.loliapi.com/acg?8" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">CMake入门</div></div></a></div><div class="next-post pull-right"><a href="/2023/12/14/26464/" title="筛去重复元素"><div class="cover" style="background:var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">筛去重复元素</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/head.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">ZU Quanzhi</div><div class="author-info__description">My personal blog is a lively corner where I document my journey of self-improvement, continuous learning, and share the highs and lows of my life's adventures. It's a digital diary where I enthusiastically capture the steps I take to better myself, the invaluable lessons I've learned, and a platform to express my thoughts and experiences.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">52</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">41</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zuquanzhi"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/zuquanzhi" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="/zuweicun@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Follow 再看 ，养成习惯</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99%EF%BC%9A"><span class="toc-number">1.</span> <span class="toc-text">参考资料：</span></a></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2"><span class="toc-number"></span> <span class="toc-text">1.发展历史</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%B8%BB%E4%BD%93%E6%B5%81%E7%A8%8B"><span class="toc-number"></span> <span class="toc-text">2.主体流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%9F%A9%E9%98%B5%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number"></span> <span class="toc-text">3.矩阵的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%E4%B8%8E%E5%B9%B3%E5%8F%B0"><span class="toc-number">0.1.</span> <span class="toc-text">学习资料与平台</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number"></span> <span class="toc-text">4.梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%EF%BC%9A"><span class="toc-number">1.</span> <span class="toc-text">梯度：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%9A"><span class="toc-number">2.</span> <span class="toc-text">梯度下降法：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number"></span> <span class="toc-text">5.反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number"></span> <span class="toc-text">定义</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">0.1.</span> <span class="toc-text">正向传播</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">0.2.</span> <span class="toc-text">反向传播</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%80%9C%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E2%80%9D%E6%B1%82%E6%8D%9F%E5%A4%B1%EF%BC%8C%E2%80%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E2%80%9D%E5%9B%9E%E4%BC%A0%E8%AF%AF%E5%B7%AE%E3%80%82"><span class="toc-number">1.</span> <span class="toc-text">“正向传播”求损失，“反向传播”回传误差。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%88%E5%85%B6%E5%AE%9E%E8%BF%99%E4%BA%9B%E7%A8%80%E5%A5%87%E5%8F%A4%E6%80%AA%E7%9A%84%E5%85%AC%E5%BC%8F%E5%B7%B2%E7%BB%8F%E7%9C%8B%E4%B8%8D%E6%87%82%E4%BA%86%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">（其实这些稀奇古怪的公式已经看不懂了）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number"></span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C%E5%AE%9E%E4%BE%8B"><span class="toc-number"></span> <span class="toc-text">操作实例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%B1%BB%EF%BC%9A"><span class="toc-number">0.1.</span> <span class="toc-text">神经网络类：</span></a></li></ol></li></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/09/27/44434/" title="gin的中间件机制"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://www.loliapi.com/acg?89" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="gin的中间件机制"></a><div class="content"><a class="title" href="/2025/09/27/44434/" title="gin的中间件机制">gin的中间件机制</a><time datetime="2025-09-27T07:19:44.000Z" title="发表于 2025-09-27 15:19:44">2025-09-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/22/59604/" title="深入解析Go中Slice底层实现"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://www.loliapi.com/acg?29" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="深入解析Go中Slice底层实现"></a><div class="content"><a class="title" href="/2025/09/22/59604/" title="深入解析Go中Slice底层实现">深入解析Go中Slice底层实现</a><time datetime="2025-09-22T05:17:44.000Z" title="发表于 2025-09-22 13:17:44">2025-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/17/7375/" title="搭建Docker私有镜像站"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://www.loliapi.com/acg?67" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="搭建Docker私有镜像站"></a><div class="content"><a class="title" href="/2025/09/17/7375/" title="搭建Docker私有镜像站">搭建Docker私有镜像站</a><time datetime="2025-09-17T10:44:21.000Z" title="发表于 2025-09-17 18:44:21">2025-09-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/17/63383/" title="cpolar实现内网穿透"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://www.loliapi.com/acg?34" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="cpolar实现内网穿透"></a><div class="content"><a class="title" href="/2025/09/17/63383/" title="cpolar实现内网穿透">cpolar实现内网穿透</a><time datetime="2025-09-17T10:30:30.000Z" title="发表于 2025-09-17 18:30:30">2025-09-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/11/43814/" title="Agent时代基础设施--MCP协议介绍"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://imgapi.jinghuashang.cn/random?1213" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Agent时代基础设施--MCP协议介绍"></a><div class="content"><a class="title" href="/2025/04/11/43814/" title="Agent时代基础设施--MCP协议介绍">Agent时代基础设施--MCP协议介绍</a><time datetime="2025-04-10T16:00:06.000Z" title="发表于 2025-04-11 00:00:06">2025-04-11</time></div></div></div></div></div></div></main><footer id="footer" style="background:url(/img/footer_bg.jpg)"><div id="footer-wrap"><div id="ft"><div class="ft-item-1"><div class="t-top"><div class="t-t-l"><p class="ft-t t-l-t">百忙之中，下一步闲棋是很有必要的</p><div class="bg-ad"><div>畏首畏尾，身余几何？</div><div class="btn-xz-box"></div></div></div></div></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@4.13.0/source/js/utils.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@4.13.0/source/js/main.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@4.13.0/source/js/tw_cn.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.8/dist/lazyload.iife.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@4.13.0/source/js/search/local-search.min.js"></script></div></div></body></html>